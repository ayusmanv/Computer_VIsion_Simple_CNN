{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Classification using Convolutional Neural Network\n",
    "#### CNN\n",
    "The dataset I am using is the popular dataset of images of dogs and cats. Although, this dataset is a simple dataset, the practical application of this model can be used for many real world application. For example, it can be used for identifying diseases if we have any medical images, like x-ray images, or MRI images. Other examples can be identifying counterfeits. The metrics for measurement of this model would be accuracy."
   ],
   "id": "5076a5624921507f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Importing Necessary Packages\n",
    "I am using Keras Library for building this CNN model"
   ],
   "id": "7dd714b55276c73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:05.960228Z",
     "start_time": "2024-10-08T00:31:03.377820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import utils\n",
    "from keras.models import load_model"
   ],
   "id": "2b1e9d9c9dec85be",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Data Preprocessing\n",
    "Building the Training set. The parameter shear_range, zoom_range, horizontal flip are for feature augmentation. Rescale is for feature scaling. This should convert the input images into matrix of to 1s and 0s."
   ],
   "id": "ca269b631a9cf21a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:25.641917Z",
     "start_time": "2024-10-08T00:31:25.414216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, #feature scaling of pixels\n",
    "                                   shear_range=0.2, #transformation for image augmentation\n",
    "                                   zoom_range=0.2, #transformation for image augmentation\n",
    "                                   horizontal_flip=True)#transformation for image augmentation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/training_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ],
   "id": "aa115e3160309810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building the test set, in test set I am not applying the transformations to avoid overfitting. Only feature scaling\n",
   "id": "d45a0647f41f594e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:29.949147Z",
     "start_time": "2024-10-08T00:31:29.882001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'data/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ],
   "id": "e97c42026d31a802",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Building\n",
    "Initializing the CNN has following steps:\n",
    "1. Adding Convolution Layer\n",
    "2. Pooling\n",
    "3. Flattening\n",
    "4. Full Connection\n",
    "5. Output Layer\n",
    "\n",
    "Note** 32 is number of feature detector, (3,3) is the size, 3 in the input shape represent colored image"
   ],
   "id": "3997f47607376172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:33.720267Z",
     "start_time": "2024-10-08T00:31:33.636113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))"
   ],
   "id": "33aaa2455f2b94a7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:35.533394Z",
     "start_time": "2024-10-08T00:31:35.515684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#pooling\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2, 2),strides=(2, 2)))"
   ],
   "id": "ff4d424347fdc5cc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:37.519Z",
     "start_time": "2024-10-08T00:31:37.495743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#second Convolution Layer\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2, 2)))"
   ],
   "id": "3a5772699d8d6053",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:39.349168Z",
     "start_time": "2024-10-08T00:31:39.337393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ],
   "id": "4abc0cc48d53addb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:40.716297Z",
     "start_time": "2024-10-08T00:31:40.688082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Building the fully connected model\n",
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))"
   ],
   "id": "1dff8d10edd41fbc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "activation function is sigmoid as it's a binary classifier and one neuron\n",
   "id": "4e5ef1b058a2968d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:42.792382Z",
     "start_time": "2024-10-08T00:31:42.772738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#output layer\n",
    "cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ],
   "id": "c9de09115fc0a895",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Fitting and Training\n",
    "The metrics used is accuracy. Which is Correct Classification/ Total Classification. "
   ],
   "id": "44db7c4a19ebcd2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:31:44.787019Z",
     "start_time": "2024-10-08T00:31:44.764175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#compile\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "id": "aee1942f7d1f8057",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:45:50.141709Z",
     "start_time": "2024-10-08T00:31:46.019192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#fit\n",
    "cnn.fit(x=train_generator, validation_data= test_generator,epochs=30)"
   ],
   "id": "e0397b78d820b831",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 67s 266ms/step - loss: 0.6665 - accuracy: 0.5914 - val_loss: 0.6239 - val_accuracy: 0.6490\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 0.6133 - accuracy: 0.6633 - val_loss: 0.5874 - val_accuracy: 0.6725\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.5630 - accuracy: 0.7153 - val_loss: 0.5397 - val_accuracy: 0.7380\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.5356 - accuracy: 0.7320 - val_loss: 0.5167 - val_accuracy: 0.7530\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.5044 - accuracy: 0.7517 - val_loss: 0.5150 - val_accuracy: 0.7580\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.4862 - accuracy: 0.7669 - val_loss: 0.5040 - val_accuracy: 0.7620\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.4714 - accuracy: 0.7745 - val_loss: 0.4825 - val_accuracy: 0.7765\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.4597 - accuracy: 0.7793 - val_loss: 0.4780 - val_accuracy: 0.7835\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.4393 - accuracy: 0.7966 - val_loss: 0.4588 - val_accuracy: 0.7930\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.4247 - accuracy: 0.8024 - val_loss: 0.4486 - val_accuracy: 0.7935\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.4082 - accuracy: 0.8064 - val_loss: 0.4758 - val_accuracy: 0.7755\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.3929 - accuracy: 0.8221 - val_loss: 0.4488 - val_accuracy: 0.7965\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3795 - accuracy: 0.8291 - val_loss: 0.4473 - val_accuracy: 0.8005\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3681 - accuracy: 0.8355 - val_loss: 0.4478 - val_accuracy: 0.8005\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3480 - accuracy: 0.8480 - val_loss: 0.4638 - val_accuracy: 0.8030\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3383 - accuracy: 0.8475 - val_loss: 0.4535 - val_accuracy: 0.8060\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3210 - accuracy: 0.8600 - val_loss: 0.4601 - val_accuracy: 0.8000\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3209 - accuracy: 0.8565 - val_loss: 0.4679 - val_accuracy: 0.8070\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3063 - accuracy: 0.8654 - val_loss: 0.4357 - val_accuracy: 0.8170\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.2951 - accuracy: 0.8739 - val_loss: 0.4507 - val_accuracy: 0.8115\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2905 - accuracy: 0.8723 - val_loss: 0.4799 - val_accuracy: 0.8070\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2695 - accuracy: 0.8879 - val_loss: 0.4365 - val_accuracy: 0.8295\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2637 - accuracy: 0.8882 - val_loss: 0.4516 - val_accuracy: 0.8175\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2522 - accuracy: 0.8960 - val_loss: 0.4685 - val_accuracy: 0.8145\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2503 - accuracy: 0.8959 - val_loss: 0.5022 - val_accuracy: 0.8130\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2254 - accuracy: 0.9062 - val_loss: 0.5057 - val_accuracy: 0.8095\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2316 - accuracy: 0.9045 - val_loss: 0.5076 - val_accuracy: 0.8055\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2139 - accuracy: 0.9085 - val_loss: 0.4935 - val_accuracy: 0.8115\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2091 - accuracy: 0.9146 - val_loss: 0.5089 - val_accuracy: 0.8160\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.1995 - accuracy: 0.9210 - val_loss: 0.5704 - val_accuracy: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2f6f19be0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The final accuracy of the model with the full dataset is 92.10%.",
   "id": "74aa46ebfd2c7448"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Saving the model\n",
    "Saving the model. This should help us use the model locally later. As well to move it to ec2 for model deployment."
   ],
   "id": "cd4b925c06cb39c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T01:23:33.892998Z",
     "start_time": "2024-10-08T01:23:33.829746Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.save('models/cnn.h5')",
   "id": "45a34a626394839",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Local Function to use the model",
   "id": "df2594aca7d7a3a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T01:23:38.017966Z",
     "start_time": "2024-10-08T01:23:38.004288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#prediction\n",
    "def predict_cats_dogs(image_path, model_path = 'models/cnn.h5'):\n",
    "    \"\"\"\n",
    "    This function will predict if the image is of a dog or cat\n",
    "    :param image_path: the path of the image \n",
    "    :param model_path: the path of the model\n",
    "    :return: the prediction\n",
    "    \"\"\"\n",
    "    predictor = load_model(model_path)\n",
    "    test_image = utils.load_img(image_path, target_size=(64, 64))\n",
    "    test_image = utils.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    predictions = predictor.predict(test_image)\n",
    "    #print(train_generator.class_indices)\n",
    "    #train_generator.class_indices\n",
    "    #print(predictions)\n",
    "    if predictions[0][0] == 1:\n",
    "        return f\"It's a Dog\"\n",
    "    else:\n",
    "        return f\"It's Cat\"\n",
    "\n"
   ],
   "id": "fa756e91c1ad6f90",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Using the model",
   "id": "4ceb5858c045c305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T01:24:13.524971Z",
     "start_time": "2024-10-08T01:24:13.307239Z"
    }
   },
   "cell_type": "code",
   "source": "predict_cats_dogs('data/real_prediction/Rando_Cat.jpg')",
   "id": "818c1281d3af4fa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's Cat\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
